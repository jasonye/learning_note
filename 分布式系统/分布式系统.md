[toc]   

# 简述

## 重点论文

推荐多次阅读的论文：

2PC和3PC介绍的论文：https://www.cnblogs.com/bangerlee/p/6216997.html

EBay最终一致性论文：https://queue.acm.org/detail.cfm?id=1394128

Raft算法论文：https://raft.github.io/

Paxos算法：



** 分布式系统的核心就是解决一个问题：不同节点间如何达成共识。（一致性问题）**， 但是分布式环境的问题：上面的看似简单的问题（不同节点间如何达成共识），因网络丢包、节点宕机恢复等场景变得复杂，由此才衍生出很多概念、协议和理论。



    为探究共识问题最大能解决的程度，于是有FLP、CAP边界理论；
    为在特定条件和范围内解决该问题，于是有一致性协议Paxos、Raft、Zab和Viewstamped Replication；
    为构建这些协议，于是有多数派、Leader选举、租约、逻辑时钟等概念和方法。



2016年围绕“不同节点如何达成共识”这个问题，加入自己的认识和理解后有下面7篇小结：

* 一致性、2PC和3PC：分布式系统2阶段提交、3阶段提交的原理
* 选举、多数派和租约
* 时间、时钟和事件顺序
* CAP：         一致性、可用性、网络分割性 只能满足两个。
* Paxos、
* Raft、Zab、
* Paxos的变种

* BASE理论、FLP理论


   看似简单的问题因网络丢包、节点宕机恢复等场景变得复杂，由此才衍生出很多概念、协议和理论。为探究共识问题最大能解决的程度，于是有FLP、CAP边界理论；为在特定条件和范围内解决该问题，于是有一致性协议Paxos、Raft、Zab和Viewstamped Replication；为构建这些协议，于是有多数派、Leader选举、租约、逻辑时钟等概念和方法。



# 现实中的问题

在系统设计时候，需要考虑的现实中问题：

1. 网络丢包、拥塞、网络数据同步延迟，以及间接引发消息传递无序。
2. 网络分化(network partition): 网络链路出现问题，将N个节点隔离成多个部分
3. 主从同步延迟、不一致等原因
4. 机器系统时钟不准、不一致
5. 节点宕机、包含宕机后恢复、
6. GC（结点GC引起的STW）
7. 拜占庭将军问题(byzantine failure)[2]: 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息


# 什么是一致性问题？

    何为一致性问题？简单而言，**一致性问题就是相互独立的节点之间如何达成一项决议的问题。**分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。比如数据库中：事务的一致性：ACID：Atomicity、Consistency、Islation、Durability、


1. 什么是一致性？
假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性：
全认同(agreement): 所有N个节点都认同一个结果
值合法(validity): 该结果必须由N个节点中的节点提出
可结束(termination): 决议过程在一定时间内结束，不会无休止地进行下去

一致性还具备两个属性，一个是强一致(safety)，它要求所有节点状态一致、共进退；一个是可用(liveness)，它要求分布式系统24*7无间断对外服务。

**根据对一致性要求的强弱程度不同，出现了各种不同的工程解决方案，按前弱程度分为：强一致性、弱一致性、和最终一致性；工程领域主要讨论的是强一致性和最终一致性的解决方案。**
具体见：[01]各类一致性对比小结

比如解决强一致性的：2PC、3PC、
解决线性一致性：
解决最终一致性：TCC、

### 2. 一致性的类型
	分布式环境下，更新同一个数据，在客户端看到的数据，保证一致性，根据不同强度，有多个不同的一致性。
	
* 因果一致性、
* Session一致性、
* 最终一致性、

分布式环境，多个Node节点之间数据一致:
* 多副本一致性
* 一致性Hash、


## 一致性问题的理论

## CAP定理
在分布式系统中，一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）3 个要素最多只能同时满足两个，不可兼得。其中，分区容忍性又是不可或缺的。

一致性：更新操作完成后，所有结点的数据保持一致。
可用性：
分区容错性：


## CAP之间的取舍
放弃分区容错性：将所有的事务都放到一台机器上，从分布式系统退化为单机系统，从根本上放弃了可扩展性。
放弃一致性：
放弃可用性：保证完全可以用

不同的软件选择了不同的方案：
举例：Cassandra、Dynamo 等，默认优先选择AP，弱化C；HBase、MongoDB 等，默认优先选择CP，弱化A。

## BASE理论
BASE 理论告诉我们：可以通过放弃系统在每个时刻的强一致性来换取系统的可扩展性。

核心思想：
•  基本可用（Basically Available）：指分布式系统在出现故障时，允许损失部分的可用性来保证核心可用。
•  软状态（Soft State）：指允许分布式系统存在中间状态，该中间状态不会影响到系统的整体可用性。
•  最终一致性（Eventual Consistency）：指分布式系统中的所有副本数据经过一定时间后，最终能够达到一致的状态。

# 工程解决方案

* 2PC、
* 3PC

#  2PC 两阶段提交协议

* 什么是2PC？
		先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。 分为：提议和执行两个阶段。 在阶段2中，coordinator根据participant的反馈，提交或中止事务，**如果participant全部同意则提交，只要有一个participant不同意就中止。**
		保证强一致性


## 这里思考的问题：
### 1. 如果coordinator宕机了怎么办？

按当即时机上有：
* 在刚发完命令后：参与者回复提议后，就挂起，收不到阶段二的提交、
* 在接收到部分回复后：参与者挂起、
* 在接收到全部回复后：参与者挂起
* 在阶段二下达命令后：参与者可以完成提交，但是没有收到反馈

    解决办法：加入一个备份的协调者、备份的协调者需要watchdog检测到主节点的状态；同时这也要求 coordinator/participant 记录(logging)历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态。

### 2. 假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机、
那么上面的备份coordinator查询不到participant的列表和状态、导致所有的主机阻塞住。

思考解决方案：需要备份着查询log，查看有多少个参与者等。


## 缺点：
   * 2PC方案的问题：

    1. 同步阻塞：所有节点是同步阻塞，性能是最大的问题、失败的概率非常的大、
    2. 数据不一致。
    3. 容易存在单点问题：主节点只有一个，非常容易成为薄弱点、

 






https://en.wikipedia.org/wiki/Two-phase_commit_protocol    
    
# 3PC：三阶段提交协议 
3阶段：
[3PC图](https://images2015.cnblogs.com/blog/116770/201603/116770-20160314002734304-489496391.png)

三个阶段：
1. 询问是否可以commit阶段
2. 准备commit阶段，协调者要求参与者锁定资源，并且资源可以回滚。
3. do commit阶段，协调者发送提交、或者取消提交给参与者、

coordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。
participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。
coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。


## 可能的问题：
1. 协调者当机：在发起提议、收到部分返回，提交准备commit
2. 参与者单机：


思考问题
1. 2pc、3pc区别？ 
* 回答：3pc多一步「预确认」；都有单点问题 点评：基本OK *
2. 如果在各个不同时机，有宕机，需要怎么处理？

一致性协议和两阶段提交：https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/
参考文档:https://www.the-paper-trail.org/post/2008-11-29-consensus-protocols-three-phase-commit/


# Raft和Paxos算法
三阶段提交也存在一些缺陷，要彻底从协议层面避免数据不一致，可以采用**Paxos或者Raft 算法**。

## Raft
分布式系统采用Paxos或者Raft协议来保证数据的强一致。

Raft将分布式一致性问题分成了三个步骤解决，包括：
* 0Leader的选举过程：设置一个随机超时时间、发起Leader选举
* 0Log的复制方案
* 0数据安全（其实就是一致性）


**优秀的解析文档
https://raft.github.io/、
https://raft.github.io/raft.pdf**

Raft协议演示过程：http://thesecretlivesofdata.com/raft/


https://pingcap.com/blog-cn/tidb-internal-1/

Etcd的Raft实现
https://cloud.tencent.com/developer/article/1394643

Raft 一致性协议：
http://km.oa.com/articles/show/358879?kmref=dailymail_headline&jumpfrom=daily_mail 

## Paxos及其变种和优化
Paxos：无比抽象的Paxos协议

**TCD使用[Raft]协议， ZK使用ZAB（类PAXOS协议）**
缓存，读写分离，分布式事务，2PC，3PC，最终一致性，paxos，一致性hash……
分布式、高可用、大数据、



## 分布式系统一致性问题

“数据一致”和“视图一致”。
我们常见的主备、raft解决的是“数据一致”，即数据在副本之间一致的持久化。
而“视图一致”则要求更高，它要求不同副本之间对数据的可见性是一致的，即题主提出的这个问题，不通入口上读取的数据严格保证不落后主机，oracle rac，purescale这种对等集群架构能够解决“视图一致”，而rac本质上是缓存sharding，purescale是单点缓存，可用性依赖底层共享存储。

basic paxos协议通过在每次数据读取时都走一遍完整投票协议，来在副本之间实现“视图一致”，可用性高，而问题是性能太差。所以在它基础上发展的raft multipaxos，通过选唯一leader的方式来重点解决数据一致，不去解决视图一致的问题。
副本之间的视图一致本质上是分布式事务的一致性读，truetime的出现也提供了一个方向，但问题依然是性能，spanner写后一致性读的延迟比较高。
所以在保证数据一致的基础上，视图一致、可用性、性能，很难兼顾，一般来说都是放弃备机的视图一致性。


# 选主策略（Leader选举） 
	目前业界内常用算法有基于序号选主的算法（Bully算法）、多数派选主算法（Raft算法和ZAB算法）等

## Raft、

## Zab（Zookeeper方法）


ZooKeeper使用了Zab协议。Zab可以分解成discovery、sync、broadcast三个阶段。
**Raft与Paxos协议类似，在容错性、性能方面相同，但是Raft把一致性问题分解为多个互相独立的子问题。**

状态机：
	Raft协议把每台机器考虑为一个状态机，如果每台机器初始状态一致，并且执行相同的状态序列，那么最终结果也一定是一致的。
	
Leader选举
	Raft算法将Server划分为3种角色。
	Leader：
		负责和Client交互和Log复制，同一时间最多存在一个。
	Follower：
		被动相应请求RPC，从不主动发起请求RPC
	Candidate：
		由Follower向Leader转换的中间状态。
        
        


# 分布式事务的解决方案
X/Open组织提出的分布式事务的规范XA，定义了分布式事务的参与者和需要实现的接口。

基础： 分布式事务规范：XA
XA定义了事务参与者的几个角色：
* AP：应用程序：
* RM：资源管理器：
* TM：事务管理器：

XA事务定义了两个接口：
TX接口：用于AP和TM之间沟通：开始事务、提交事务、回滚事务；
XA接口：用于TM和RM之间沟通，用于事务管理器将志愿管理器加入事务


分布式事务的实现主要有以下 5 种方案：
* 2PC解决方案（XA 方案）
* TCC 方案
* 本地消息表（异步确保)来自Ebay、然后通过消息队列发送出去、
* 可靠消息最终一致性方案（基于MQ实现的，比如阿里的RocketMQ实现的方式）
* Sagas事务模型：最大努力通知方案


对于分布式事务，为了发挥若星

## 1. 基于XA的2PC
两阶段提交：第一阶段：所有参与者进入准备状态 Prepare，并且返回成功或者失败、第二阶段：提交、或者Cancel阶段、
两阶段提交的过程、
三阶段提交的过程、

两阶段事务性能比较差，效率不够高，所有引入了柔性事务。

## 2. 最终一致性解决方案：TCC：Try-Confirm-Cancel
在服务端定义三个接口，
Try接口试图执行事务操作能否完成，并执行锁定资源；
Confirm接口执行真正的事务操作。
Cancel接口，执行返回、取消事务操作



## 3. 本地消息表
本地一个表完成事务、

## 4. 基于MQ消息队列的分布式事务
阿里的RocketMQ解决方案。
1. 消息队列提供一个”事务消息“，这个消息被提交后并不会被下游消费者消费到。
2. 消息队列过提供过一定时间后回查消息状态功能。

流程：
1. 本地事务发起者，先发送一个”事务消息“
2. 然后执行本地事务，本地事务执行成功、或者失败后，提交给MQ Svr提交事务、或者Canncel事务、
3. MQ如果在操作一定时间后没有收到提交操作，会回查消息发送者接口，查询成功还是失败，确定提交还是回滚
4. MQ Svr根据2、3的结果，执行提交或者回滚操作。

## 5. Sagas事务模型（最终一致性）
一、事件/编排：没有中央协调器，每个服务产生并聆听其他事务的事件，并决定是否应该采取行动
二、命令/协调：中央协调器负责集中处理事件的决策和业务逻辑排序。






# 分布式锁的理论
https://blog.csdn.net/canot/article/details/100569263

谈谈对分布式锁理解, 用什么方式实现, 以及加锁的实现过程？

# 面试题：
1. 分布式算法有哪些？zookeeper、consul、redis cluster方案分别用的哪一种？

# 学习计划：
1. 熟悉分布式系统原理
1. 学习现有分布式系统源代码：比如微信的开源代码、etcd代码实现
2. 参与到一个开源项目中去

 




学习NSQ分布式系统的实现、
学习分布式文件系统的实现
学习etcd分布式服务发现的实现方法
跟踪系统

要开发自己的轮子！而且是要有人用的轮子：
做一个自己的东西、即使是

分布式文件系统
https://github.com/chrislusf/seaweedfs
Jaeger, a Distributed Tracing System
https://github.com/uber/jaeger
微服务框架
https://github.com/micro/micro




大佬：你有写过分布式的业务吗？
我：我写过一个基于HDFS分布式存储的KVStore，上层使用Hadoop的API实现。
大佬：那个是分布式存储，我想了解一下分布式业务？
我：（赶紧纠正）那dubbo算吗？（于是介绍了一下自己了解的dubbo）
大佬：ok。那你觉得分布式的话会遇到什么问题呢？
我：那就是经典的CAP问题了。没有数据库能够同时满足这三个问题
大佬：那你能具体解释一下CAP代表什么吗？
我：（紧张到一片空白）Consistency？Atomic？P…Persistency？？？
大佬：。。。。。。
CAP：是指Consistency一致性，Availability可用性，Partition Tolerance分区容忍性





分布式系统的服务发现、容器调度、分布式数据存储、Leader选举 、分布式锁、检测机器可用性等。

These are systems that will never tolerate split-brain operation and are willing to sacrifice availability to achieve this end.

etcd和consul、zookeeper的比较
etcd：解决配置问题、
consul：解决服务发现的问题、


②具有大规模分布式系统的调优经验；
③熟悉大规模分布式系统架构设计，熟悉CAP、Quorum、Consistent Hashing等原理和算法；

 


# 参考文献
https://www.cnblogs.com/bangerlee/p/5268485.html
